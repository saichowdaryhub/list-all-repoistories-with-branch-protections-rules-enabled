name: Branch Protection Detailed Audit (Rules API + Slack)

on:
  workflow_dispatch:

jobs:
  audit:
    runs-on: ubuntu-latest
    permissions:
      contents: read

    env:
      ORG_NAME: ${{ github.repository_owner }}
      GITHUB_TOKEN: ${{ secrets.BP_PAT }}
      SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK_URL }}
      SLACK_BOT_TOKEN: ${{ secrets.SLACK_BOT_TOKEN }}
      SLACK_CHANNEL: ${{ secrets.SLACK_CHANNEL }}
      DEBUG: "true"
      PYTHONUNBUFFERED: "1"

    steps:
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.x"

      - name: Install dependencies
        run: pip install requests

      - name: Run Unified Branch Protection Audit
        run: |
          python - <<'EOF'
          import os, requests, time

          ORG = os.getenv("ORG_NAME")
          TOKEN = os.getenv("GITHUB_TOKEN")
          SLACK = os.getenv("SLACK_WEBHOOK")
          DEBUG = os.getenv("DEBUG","false").lower()=="true"

          def log(m,l="INFO"): print(f"[{l}] {m}", flush=True)
          def dbg(m): 
              if DEBUG: log(m,"DEBUG")

          H = {"Authorization": f"Bearer {TOKEN}", "Accept": "application/vnd.github+json"}

          def get_json(url):
              r = requests.get(url, headers=H, timeout=30)
              dbg(f"GET {url} -> {r.status_code}")
              if r.status_code==200: return r.json()
              if r.status_code==404: return None
              log(f"Failed GET {url} -> {r.status_code}: {r.text[:150]}", "WARN")
              return None

          def list_repos():
              repos, page = [], 1
              while True:
                  # Use visibility=private parameter to filter at API level
                  data = get_json(f"https://api.github.com/orgs/{ORG}/repos?per_page=100&page={page}&type=all&visibility=private")
                  if not data: break
                  repos += data
                  if len(data) < 100: break
                  page += 1
              # Filter to exclude archived repositories
              active_private_repos = [r for r in repos if not r.get("archived", False)]
              return active_private_repos

          def get_rules(repo, branch):
              return get_json(f"https://api.github.com/repos/{ORG}/{repo}/rules/branches/{branch}") or []

          def try_get_classic_protection(repo, branch):
              # Best-effort classic protection fetch; may return 403 if PAT lacks scope
              url = f"https://api.github.com/repos/{ORG}/{repo}/branches/{branch}/protection"
              r = requests.get(url, headers=H, timeout=30)
              dbg(f"GET {url} -> {r.status_code}")
              if r.status_code == 200:
                  return r.json()
              return None

          # ---- Start
          repos = list_repos()
          log(f"Total active private repositories found: {len(repos)}")
          for r in repos: log(f"→ {r['name']}")

          missing, ok = [], []
          repo_summaries = []  # per-repo tabular summary rows

          for idx, repo in enumerate(repos, start=1):
              name = repo["name"]
              default_branch = repo.get("default_branch", "main")
              log(f"\n[{idx}/{len(repos)}] Checking repository: {name}")
              log(f"Default branch for {name} is '{default_branch}'")

              issues = []
              
              # Step 2: Check rules on default branch directly using Rules API (works with RULESET_PAT)
              rules = get_rules(name, default_branch)
              
              if not rules:
                  # Rules API returned empty - try classic protection endpoint as fallback
                  classic = try_get_classic_protection(name, default_branch)
                  if classic:
                      issues = []
                      pr_reviews = classic.get("required_pull_request_reviews")
                      if not pr_reviews:
                          issues.append("Require pull request before merging not enabled")
                      else:
                          cnt = pr_reviews.get("required_approving_review_count", 0)
                          if int(cnt) != 2:
                              issues.append(f"Required approvals = {cnt}, expected 2")

                      if issues:
                          log(f"{name}: ❌ Issues (classic branch protection) → {', '.join(issues)}")
                          missing.append({"repo": name, "issues": issues, "source": "classic"})
                          repo_summaries.append({
                              "repo": name,
                              "default": default_branch,
                              "has_protection": True,
                              "require_pr": (pr_reviews is not None),
                              "approvals_ok": (pr_reviews and pr_reviews.get("required_approving_review_count", 0) == 2),
                              "restrict_deletions": False,
                              "non_fast_forward": False,
                          })
                      else:
                          log(f"{name}: ✅ All required rules in place (classic branch protection)")
                          ok.append(name)
                          repo_summaries.append({
                              "repo": name,
                              "default": default_branch,
                              "has_protection": True,
                              "require_pr": True,
                              "approvals_ok": True,
                              "restrict_deletions": False,
                              "non_fast_forward": False,
                          })
                      continue
                  
                  # Workaround: If classic endpoint returned 403, check common branch names for mismatch
                  # Check master, develop, dev branches to detect misconfigured protection
                  common_branches = ["master", "develop", "dev"]
                  protected_branches = []
                  
                  for branch in common_branches:
                      if branch != default_branch:
                          classic_check = try_get_classic_protection(name, branch)
                          if classic_check:
                              protected_branches.append(branch)
                  
                  if protected_branches:
                      # Found classic protection on a different branch
                      branches_str = ", ".join(protected_branches)
                      log(f"{name}: ⚠️ Misconfigured branch protection found: Default branch is '{default_branch}' but classic branch protection is configured for {branches_str}")
                      missing.append({
                          "repo": name,
                          "issues": [f"Misconfigured branch protection found: Default branch is '{default_branch}' but classic branch protection is configured for {branches_str}"],
                          "source": "mismatch"
                      })
                      repo_summaries.append({
                          "repo": name,
                          "default": default_branch,
                          "has_protection": False,
                          "require_pr": False,
                          "approvals_ok": False,
                          "restrict_deletions": False,
                          "non_fast_forward": False,
                      })
                      continue
                  
                  # No protection found anywhere
                  log(f"{name}: ❌ No branch protection rule or ruleset found on default branch '{default_branch}'. Note: Classic branch protection may exist please check manually")
                  missing.append({
                      "repo": name,
                      "issues": [f"No branch protection rule or ruleset found on default branch '{default_branch}'. Note: Classic branch protection may exist please check manually"],
                      "source": "no protection"
                  })
                  repo_summaries.append({
                      "repo": name,
                      "default": default_branch,
                      "has_protection": False,
                      "require_pr": False,
                      "approvals_ok": False,
                      "restrict_deletions": False,
                      "non_fast_forward": False,
                  })
                  continue

              # Rules found on default branch - determine if classic or ruleset
              rule_source = None
              ruleset_name = None
              
              # Debug: log what rules we received
              dbg(f"Rules found on {default_branch}: {rules}")
              
              for rule in rules:
                  rule_type = rule.get("type")
                  rule_source_val = rule.get("source")
                  ruleset_source_type = rule.get("ruleset_source_type")
                  ruleset_source = rule.get("ruleset_source")
                  dbg(f"Rule: type={rule_type}, source={rule_source_val}, ruleset_source_type={ruleset_source_type}")
                  
                  # Check for classic branch protection
                  if rule_source_val == "BranchProtectionRule":
                      rule_source = "classic"
                      break
                  # Check for ruleset (two possible formats)
                  elif rule_source_val == "RepositoryRuleSet" or ruleset_source_type == "Repository":
                      rule_source = "ruleset"
                      # Try multiple ways to get ruleset name
                      ruleset_name = rule.get("ruleset_name") or rule.get("name") or ruleset_source or "unknown"
                      break

              if not rule_source:
                  # Rules found but we can't identify the source - provide diagnostic info
                  sources_found = [r.get("source") for r in rules if r.get("source")]
                  types_found = [r.get("type") for r in rules if r.get("type")]
                  log(f"{name}: ⚠️ Branch protection rules found but source type not recognized. Sources: {sources_found}, Types: {types_found}")
                  
                  # Still try to validate what we can
                  rule_types = {r.get("type"): r for r in rules}
                  
                  # Check for common rule types regardless of source
                  if "pull_request" not in rule_types:
                      issues.append("Require pull request before merging not enabled")
                  
                  if "required_approving_review_count" not in rule_types:
                      issues.append("Required approving reviews rule missing")
                  else:
                      params = rule_types["required_approving_review_count"].get("parameters") or {}
                      count = params.get("required_approving_review_count") or params.get("count")
                      if not count:
                          issues.append("Approvals rule present but count missing")
                      elif int(count) != 2:
                          issues.append(f"Required approvals = {count}, expected 2")
                  
                  if issues:
                      log(f"{name}: ❌ Issues found in unrecognized branch protection configuration → {', '.join(issues)}")
                      missing.append({"repo": name, "issues": issues, "source": f"unrecognized (sources: {', '.join(sources_found or ['none'])})"})
                      repo_summaries.append({
                          "repo": name,
                          "default": default_branch,
                          "has_protection": True,
                          "require_pr": ("pull_request" in rule_types),
                          "approvals_ok": ("required_approving_review_count" in rule_types and (rule_types["required_approving_review_count"].get("parameters", {}).get("required_approving_review_count") or rule_types["required_approving_review_count"].get("parameters", {}).get("count")) == 2),
                          "restrict_deletions": ("restrict_deletions" in rule_types),
                          "non_fast_forward": ("non_fast_forward" in rule_types),
                      })
                  else:
                      log(f"{name}: ✅ Rules found but source type not recognized. Types: {', '.join(types_found or ['none'])}")
                      missing.append({
                          "repo": name,
                          "issues": [f"Branch protection rules found but configuration type not recognized. Rule types found: {', '.join(types_found or ['none'])}"],
                          "source": "unrecognized"
                      })
                      repo_summaries.append({
                          "repo": name,
                          "default": default_branch,
                          "has_protection": True,
                          "require_pr": ("pull_request" in rule_types),
                          "approvals_ok": ("required_approving_review_count" in rule_types and (rule_types["required_approving_review_count"].get("parameters", {}).get("required_approving_review_count") or rule_types["required_approving_review_count"].get("parameters", {}).get("count")) == 2),
                          "restrict_deletions": ("restrict_deletions" in rule_types),
                          "non_fast_forward": ("non_fast_forward" in rule_types),
                      })
                  continue

              if rule_source == "classic":
                  log(f"{name}: Found classic branch protection rule on '{default_branch}'")
                  # Verify classic branch protection settings via Rules API
                  rule_types = {r.get("type"): r for r in rules}
                  
                  if "pull_request" not in rule_types:
                      issues.append("Require pull request before merging not enabled")

                  if "required_approving_review_count" not in rule_types:
                      issues.append("Required approving reviews rule missing")
                  else:
                      params = rule_types["required_approving_review_count"].get("parameters") or {}
                      count = params.get("required_approving_review_count") or params.get("count")
                      if not count:
                          issues.append("Approvals rule present but count missing")
                      elif int(count) != 2:
                          issues.append(f"Required approvals = {count}, expected 2")

                  if issues:
                      log(f"{name}: ❌ Issues (classic branch protection) → {', '.join(issues)}")
                      missing.append({"repo": name, "issues": issues, "source": "classic"})
                      repo_summaries.append({
                          "repo": name,
                          "default": default_branch,
                          "has_protection": True,
                          "require_pr": True,
                          "approvals_ok": ("required_approving_review_count" in rule_types and (rule_types["required_approving_review_count"].get("parameters", {}).get("required_approving_review_count") or rule_types["required_approving_review_count"].get("parameters", {}).get("count")) == 2),
                          "restrict_deletions": False,
                          "non_fast_forward": False,
                      })
                  else:
                      log(f"{name}: ✅ All required rules in place (classic branch protection)")
                      ok.append(name)
                      repo_summaries.append({
                          "repo": name,
                          "default": default_branch,
                          "has_protection": True,
                          "require_pr": True,
                          "approvals_ok": True,
                          "restrict_deletions": False,
                          "non_fast_forward": False,
                      })

              elif rule_source == "ruleset":
                  log(f"{name}: Found ruleset '{ruleset_name}' on '{default_branch}'")
                  # Ruleset matches default branch, now check the rules
                  rule_types = {r.get("type"): r for r in rules}
                  
                  if "pull_request" not in rule_types:
                      issues.append("Require pull request before merging not enabled")

                  if "required_approving_review_count" not in rule_types:
                      issues.append("Required approving reviews rule missing")
                  else:
                      params = rule_types["required_approving_review_count"].get("parameters") or {}
                      count = params.get("required_approving_review_count") or params.get("count")
                      if not count:
                          issues.append("Approvals rule present but count missing")
                      elif int(count) != 2:
                          issues.append(f"Required approvals = {count}, expected 2")

                  if "restrict_deletions" not in rule_types:
                      issues.append("Restrict deletions rule not enabled")

                  if "non_fast_forward" not in rule_types:
                      issues.append("Block force pushes rule not enabled")

                  if issues:
                      log(f"{name}: ❌ Issues (ruleset '{ruleset_name}') → {', '.join(issues)}")
                      missing.append({"repo": name, "issues": issues, "source": f"ruleset '{ruleset_name}'"})
                      repo_summaries.append({
                          "repo": name,
                          "default": default_branch,
                          "has_protection": True,
                          "require_pr": ("pull_request" in rule_types),
                          "approvals_ok": ("required_approving_review_count" in rule_types and (rule_types["required_approving_review_count"].get("parameters", {}).get("required_approving_review_count") or rule_types["required_approving_review_count"].get("parameters", {}).get("count")) == 2),
                          "restrict_deletions": ("restrict_deletions" in rule_types),
                          "non_fast_forward": ("non_fast_forward" in rule_types),
                      })
                  else:
                      log(f"{name}: ✅ All required rules in place (ruleset '{ruleset_name}')")
                      ok.append(name)
                      repo_summaries.append({
                          "repo": name,
                          "default": default_branch,
                          "has_protection": True,
                          "require_pr": True,
                          "approvals_ok": True,
                          "restrict_deletions": True,
                          "non_fast_forward": True,
                      })

          # ---- Slack reporting with smart grouping and truncation for large repos
          if SLACK:
              # Group repositories by issue type (consolidate all rulesets)
              def group_by_issue_type():
                  groups = {}
                  for m in missing:
                      source = m.get("source", "unknown")
                      # Group all rulesets together regardless of ruleset name
                      if "ruleset" in source.lower():
                          normalized_source = "ruleset"
                      else:
                          normalized_source = source
                      if normalized_source not in groups:
                          groups[normalized_source] = []
                      groups[normalized_source].append(m)
                  return groups

              # Generate CSV report with all repository data
              def generate_csv_report():
                  import csv
                  import io
                  
                  output = io.StringIO()
                  writer = csv.writer(output)
                  
                  # Write header
                  writer.writerow([
                      "Repository",
                      "Default Branch",
                      "Status",
                      "Issue Category",
                      "Protected",
                      "Require PR",
                      "Approvals=2",
                      "Restrict Deletions",
                      "Block Force Pushes",
                      "Issues"
                  ])
                  
                  # Create lookup for summaries
                  missing_names = set(m["repo"] for m in missing)
                  summary_lookup = {s["repo"]: s for s in repo_summaries}
                  
                  # Write repositories with issues
                  groups = group_by_issue_type()
                  source_labels = {
                      "no protection": "No Protection",
                      "mismatch": "Misconfigured Protection",
                      "classic": "Classic Protection Issues",
                      "ruleset": "Ruleset Issues",
                      "unrecognized": "Unrecognized Configuration"
                  }
                  
                  for source, repos_list in sorted(groups.items()):
                      category = source_labels.get(source, source)
                      for m in repos_list:
                          repo_name = m["repo"]
                          issues_str = "; ".join(m.get("issues", []))
                          
                          if repo_name in summary_lookup:
                              s = summary_lookup[repo_name]
                              writer.writerow([
                                  repo_name,
                                  s["default"],
                                  "Non-Compliant",
                                  category,
                                  "Yes" if s["has_protection"] else "No",
                                  "Yes" if s["require_pr"] else "No",
                                  "Yes" if s["approvals_ok"] else "No",
                                  "Yes" if s["restrict_deletions"] else "No",
                                  "Yes" if s["non_fast_forward"] else "No",
                                  issues_str
                              ])
                          else:
                              writer.writerow([
                                  repo_name,
                                  "unknown",
                                  "Non-Compliant",
                                  category,
                                  "No",
                                  "No",
                                  "No",
                                  "No",
                                  "No",
                                  issues_str
                              ])
                  
                  # Write compliant repositories
                  for repo_name in ok:
                      if repo_name in summary_lookup:
                          s = summary_lookup[repo_name]
                          writer.writerow([
                              repo_name,
                              s["default"],
                              "Compliant",
                              "N/A",
                              "Yes" if s["has_protection"] else "No",
                              "Yes" if s["require_pr"] else "No",
                              "Yes" if s["approvals_ok"] else "No",
                              "Yes" if s["restrict_deletions"] else "No",
                              "Yes" if s["non_fast_forward"] else "No",
                              ""
                          ])
                      else:
                          writer.writerow([
                              repo_name,
                              "unknown",
                              "Compliant",
                              "N/A",
                              "Unknown",
                              "Unknown",
                              "Unknown",
                              "Unknown",
                              "Unknown",
                              ""
                          ])
                  
                  return output.getvalue()

              # Send summary with grouped overview (all repos shown in tabular format)
              summary_block = {
                  "type": "section",
                  "text": {
                      "type": "mrkdwn",
                      "text": (
                          f"*Branch Protection Audit Summary*\n"
                          f"*Total repositories:* {len(repos)}\n"
                          f"*Compliant:* {len(ok)}\n"
                          f"*With Issues:* {len(missing)}"
                      )
                  }
              }
              
              blocks = [summary_block]
              
              if missing:
                  groups = group_by_issue_type()
                  source_labels = {
                      "no protection": "No Protection",
                      "mismatch": "Misconfigured Protection",
                      "classic": "Classic Protection Issues",
                      "ruleset": "Ruleset Issues",
                      "unrecognized": "Unrecognized Configuration"
                  }
                  
                  # Summary by issue type
                  issue_summary = []
                  for source, repos_list in sorted(groups.items()):
                      label = source_labels.get(source, source)
                      issue_summary.append(f"• *{label}:* {len(repos_list)} repositories")
                  
                  if issue_summary:
                      blocks.append({
                          "type": "section",
                          "text": {
                              "type": "mrkdwn",
                              "text": "*Issues by Category:*\n" + "\n".join(issue_summary)
                          }
                      })
                  
                  # Show repos per category in tabular format
                  missing_names = set(m["repo"] for m in missing)
                  def fmt(val):
                      return "Yes" if val else "No"
                  
                  # Create a lookup for repo summaries
                  summary_lookup = {s["repo"]: s for s in repo_summaries if s["repo"] in missing_names}
                  
                  for source, repos_list in sorted(groups.items()):
                      label = source_labels.get(source, source)
                      
                      # Build table rows for this category
                      category_rows = []
                      for r in repos_list:
                          repo_name = r["repo"]
                          if repo_name in summary_lookup:
                              s = summary_lookup[repo_name]
                              category_rows.append({
                                  "Repository": repo_name,
                                  "Default": s["default"],
                                  "Protected": fmt(s["has_protection"]),
                                  "Require PR": fmt(s["require_pr"]),
                                  "Approvals=2": fmt(s["approvals_ok"]),
                                  "Restrict deletions": fmt(s["restrict_deletions"]),
                                  "Block force pushes": fmt(s["non_fast_forward"]),
                              })
                      
                      if category_rows:
                          # Compute column widths for this category
                          headers = ["Repository", "Default", "Protected", "Require PR", "Approvals=2", "Restrict deletions", "Block force pushes"]
                          col_widths = {h: len(h) for h in headers}
                          for row in category_rows:
                              for h in headers:
                                  col_widths[h] = max(col_widths[h], len(str(row[h])))
                          
                          def format_row(row_dict):
                              return "  ".join(str(row_dict[h]).ljust(col_widths[h]) for h in headers)
                          
                          header_line = format_row({h: h for h in headers})
                          sep_line = "  ".join("-" * col_widths[h] for h in headers)
                          body_lines = [format_row(r) for r in category_rows]
                          
                          blocks.append({"type": "divider"})
                          
                          # Slack has a 3000 character limit per text block
                          # Reserve ~200 chars for label and formatting, so chunk at ~2800
                          MAX_BLOCK_CHARS = 2800
                          
                          # Calculate chars per row (approximate)
                          if not category_rows:
                              continue
                          sample_row = format_row(category_rows[0])
                          chars_per_row = len(sample_row) + 1  # +1 for newline
                          header_chars = len(header_line) + len(sep_line) + 2  # +2 for newlines
                          
                          # Estimate rows per chunk (ensure at least 1 row)
                          available_for_rows = MAX_BLOCK_CHARS - header_chars - 200
                          rows_per_chunk = max(1, min(len(body_lines), available_for_rows // chars_per_row))
                          
                          # Split into chunks
                          for chunk_idx in range(0, len(body_lines), rows_per_chunk):
                              chunk_body = body_lines[chunk_idx:chunk_idx + rows_per_chunk]
                              table_text = "\n".join([header_line, sep_line] + chunk_body)
                              
                              # Build label text
                              if len(body_lines) > rows_per_chunk:
                                  start_row = chunk_idx + 1
                                  end_row = min(chunk_idx + rows_per_chunk, len(body_lines))
                                  chunk_label = f"*{label} ({len(repos_list)} total) - Part {chunk_idx // rows_per_chunk + 1} (rows {start_row}-{end_row}):*"
                              else:
                                  chunk_label = f"*{label} ({len(repos_list)} total):*"
                              
                              # Final check: if still too long, truncate more aggressively
                              full_text = f"{chunk_label}\n```{table_text}```"
                              if len(full_text) > 3000:
                                  # Truncate table to fit
                                  available = 3000 - len(chunk_label) - 10  # -10 for code block markers
                                  header_sep_chars = len(header_line) + len(sep_line) + 2
                                  remaining = max(1, available - header_sep_chars)
                                  rows_to_show = max(1, remaining // chars_per_row)
                                  chunk_body = body_lines[chunk_idx:chunk_idx + rows_to_show]
                                  table_text = "\n".join([header_line, sep_line] + chunk_body)
                                  if chunk_idx + rows_to_show < len(body_lines):
                                      table_text += f"\n... ({len(body_lines) - chunk_idx - rows_to_show} more rows)"
                                  full_text = f"{chunk_label}\n```{table_text}```"
                              
                              # Ensure text is within Slack limits (3000 chars max)
                              if len(full_text) > 3000:
                                  # Last resort: truncate the text itself
                                  max_table_chars = 3000 - len(chunk_label) - 10
                                  if max_table_chars > 0:
                                      table_text_truncated = table_text[:max_table_chars]
                                      full_text = f"{chunk_label}\n```{table_text_truncated}...```"
                                  else:
                                      # If label is too long, use a shorter label
                                      full_text = f"*{label} ({len(repos_list)} total) - Part {chunk_idx // rows_per_chunk + 1}:*\n```{table_text[:2970]}...```"
                              
                              # Only append if we have valid text
                              if full_text and len(full_text) <= 3000:
                                  blocks.append({
                                      "type": "section",
                                      "text": {
                                          "type": "mrkdwn",
                                          "text": full_text
                                      }
                                  })
                              else:
                                  log(f"Warning: Skipping block for {label} - text too long ({len(full_text)} chars)", "WARN")
                  
                  # Add note about full report
                  blocks.append({
                      "type": "section",
                      "text": {
                          "type": "mrkdwn",
                          "text": f"*Full detailed CSV report available* ({len(missing)} repositories with issues)\nDownload from workflow artifacts: `branch-protection-report.csv`"
                      }
                  })
              else:
                  blocks.append({
                      "type": "section",
                      "text": {
                          "type": "mrkdwn",
                          "text": "*All repositories are fully compliant!*"
                      }
                  })

              # Send summary message(s)
              # Slack has a limit of 50 blocks per message
              MAX_BLOCKS_PER_MESSAGE = 50
              summary_text = f"Branch Protection Audit: {len(ok)} compliant, {len(missing)} with issues out of {len(repos)} total"
              
              if len(blocks) <= MAX_BLOCKS_PER_MESSAGE:
                  # Single message
                  payload = {"text": summary_text, "blocks": blocks}
                  try:
                      r = requests.post(SLACK, json=payload, timeout=30)
                      log(f"Slack summary POST -> {r.status_code}")
                      if r.status_code != 200:
                          log(f"Slack error response: {r.text}", "WARN")
                  except Exception as e:
                      log(f"Slack summary POST failed: {e}", "WARN")
              else:
                  # Split into multiple messages
                  # First message: summary + first batch of blocks
                  first_batch = blocks[:MAX_BLOCKS_PER_MESSAGE-1]  # -1 for summary block
                  first_batch.insert(0, {
                      "type": "section",
                      "text": {
                          "type": "mrkdwn",
                          "text": f"*{summary_text}*\n*Note: Message split due to size ({len(blocks)} blocks total)*"
                      }
                  })
                  payload = {"text": summary_text, "blocks": first_batch}
                  try:
                      r = requests.post(SLACK, json=payload, timeout=30)
                      log(f"Slack summary POST (part 1) -> {r.status_code}")
                      if r.status_code != 200:
                          log(f"Slack error response: {r.text}", "WARN")
                  except Exception as e:
                      log(f"Slack summary POST (part 1) failed: {e}", "WARN")
                  
                  # Remaining messages: chunks of MAX_BLOCKS_PER_MESSAGE blocks
                  remaining_blocks = blocks[MAX_BLOCKS_PER_MESSAGE-1:]
                  for part_idx in range(0, len(remaining_blocks), MAX_BLOCKS_PER_MESSAGE):
                      chunk = remaining_blocks[part_idx:part_idx + MAX_BLOCKS_PER_MESSAGE]
                      part_num = (part_idx // MAX_BLOCKS_PER_MESSAGE) + 2
                      chunk.insert(0, {
                          "type": "section",
                          "text": {
                              "type": "mrkdwn",
                              "text": f"*Branch Protection Audit (continued - Part {part_num})*"
                          }
                      })
                      payload = {"text": f"Branch Protection Audit (Part {part_num})", "blocks": chunk}
                      try:
                          r = requests.post(SLACK, json=payload, timeout=30)
                          log(f"Slack summary POST (part {part_num}) -> {r.status_code}")
                          if r.status_code != 200:
                              log(f"Slack error response: {r.text}", "WARN")
                          time.sleep(1)  # Small delay between messages
                      except Exception as e:
                          log(f"Slack summary POST (part {part_num}) failed: {e}", "WARN")

              # Generate CSV report and send to Slack, then upload as artifact
              csv_report = generate_csv_report()
              report_file = "branch_protection_report.csv"
              
              try:
                  # Save CSV file
                  with open(report_file, "w", encoding="utf-8") as f:
                      f.write(csv_report)
                  log(f"CSV report saved to {report_file} ({len(csv_report)} bytes)")
                  
                  # Send CSV file to Slack using files.upload API
                  # Note: This requires a Slack bot token (not webhook URL)
                  slack_bot_token = os.getenv("SLACK_BOT_TOKEN")
                  slack_channel = os.getenv("SLACK_CHANNEL", "")  # Channel ID (e.g., C1234567890)
                  
                  if slack_bot_token:
                      csv_bytes = csv_report.encode("utf-8")
                      
                      files_url = "https://slack.com/api/files.upload"
                      files_headers = {"Authorization": f"Bearer {slack_bot_token}"}
                      
                      files_data = {
                          "filename": "branch_protection_report.csv",
                          "title": f"Branch Protection Audit Report - {time.strftime('%Y-%m-%d %H:%M:%S')}",
                          "initial_comment": f"Branch Protection Audit Report: {len(ok)} compliant, {len(missing)} with issues out of {len(repos)} total repositories",
                      }
                      
                      # Add channel if provided (channel ID, not channel name)
                      if slack_channel:
                          files_data["channels"] = slack_channel
                      
                      files_files = {
                          "file": ("branch_protection_report.csv", csv_bytes, "text/csv")
                      }
                      
                      try:
                          # Use files.upload API
                          r = requests.post(files_url, headers=files_headers, data=files_data, files=files_files, timeout=60)
                          if r.status_code == 200:
                              result = r.json()
                              if result.get("ok"):
                                  file_url = result.get("file", {}).get("permalink")
                                  file_id = result.get("file", {}).get("id")
                                  log(f"CSV file uploaded to Slack: {file_url}")
                                  
                                  # Post a message with the file link
                                  file_link_block = {
                                      "type": "section",
                                      "text": {
                                          "type": "mrkdwn",
                                          "text": f"*CSV Report Available*\nDownload: <{file_url}|branch_protection_report.csv>"
                                      }
                                  }
                                  file_payload = {"text": "CSV Report", "blocks": [file_link_block]}
                                  r2 = requests.post(SLACK, json=file_payload, timeout=30)
                                  log(f"Slack file link POST -> {r2.status_code}")
                              else:
                                  log(f"Slack files.upload error: {result.get('error', 'unknown')}", "WARN")
                          else:
                              log(f"Slack files.upload HTTP error: {r.status_code} - {r.text[:200]}", "WARN")
                      except Exception as e:
                          log(f"Slack file upload failed: {e}", "WARN")
                  else:
                      log("SLACK_BOT_TOKEN not set - CSV file will only be available as artifact. To send CSV to Slack, set SLACK_BOT_TOKEN secret with 'files:write' scope.", "WARN")
                      # Post a note about CSV in the message
                      csv_note = {
                          "type": "section",
                          "text": {
                              "type": "mrkdwn",
                              "text": f"*CSV report generated* ({len(csv_report)} bytes)\nDownload from workflow artifacts: `branch-protection-report.csv`"
                          }
                      }
                      try:
                          r = requests.post(SLACK, json={"text": "CSV Report", "blocks": [csv_note]}, timeout=30)
                          log(f"Slack CSV note POST -> {r.status_code}")
                      except Exception as e:
                          log(f"Slack CSV note POST failed: {e}", "WARN")
              except Exception as e:
                  log(f"Failed to save CSV report file: {e}", "WARN")
          else:
              log("No SLACK_WEBHOOK set, skipping Slack notification.", "WARN")
          EOF

      - name: Upload CSV Report Artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: branch-protection-report
          path: branch_protection_report.csv
          retention-days: 30
          if-no-files-found: ignore